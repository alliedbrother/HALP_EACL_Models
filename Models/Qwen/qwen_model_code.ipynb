{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJIKBAPzHZEW"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import traceback\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.multiprocessing as mp\n",
        "import warnings\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def check_gpu_availability():\n",
        "    \"\"\"Check GPU availability and set appropriate device.\"\"\"\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            device_count = torch.cuda.device_count()\n",
        "            print(f\"CUDA is available with {device_count} GPU(s)\")\n",
        "\n",
        "            # Try to create a tensor on GPU to test functionality\n",
        "            test_tensor = torch.tensor([1.0]).cuda()\n",
        "            del test_tensor\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            return \"cuda\"\n",
        "        else:\n",
        "            print(\"CUDA is not available\")\n",
        "            return \"cpu\"\n",
        "    except Exception as e:\n",
        "        print(f\"GPU test failed: {e}\")\n",
        "        print(\"Falling back to CPU\")\n",
        "        return \"cpu\"\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 1  # Qwen2.5-VL requires batch size 1 for vision processing\n",
        "PART_SIZE = 250  # Images per part file (as requested)\n",
        "NUM_WORKERS = 0  # Disabled multiprocessing for Qwen2.5-VL\n",
        "IMAGE_DIR = \"/root/akhil_workspace/coco_val2014\"\n",
        "OUTPUT_DIR = \"/root/akhil_workspace/qwen_model/extracted_embeddings_coco2014\"\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"  # Can also use \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "# Test mode - set to True to process only 2 images for testing\n",
        "TEST_MODE = True  # Set to False for full processing\n",
        "\n",
        "# Global model variables (will be initialized in main)\n",
        "processor = None\n",
        "model = None\n",
        "device = None\n",
        "\n",
        "def get_model_architecture_info(model):\n",
        "    \"\"\"Get the total number of layers in the language model.\"\"\"\n",
        "    arch_info = {}\n",
        "\n",
        "    # Get language model layers\n",
        "    if hasattr(model, 'model') and hasattr(model.model, 'config'):\n",
        "        config = model.model.config\n",
        "        arch_info['language_layers'] = getattr(config, 'num_hidden_layers', None)\n",
        "        arch_info['language_hidden_size'] = getattr(config, 'hidden_size', None)\n",
        "\n",
        "    # Get vision model info\n",
        "    if hasattr(model, 'visual'):\n",
        "        arch_info['has_vision_tower'] = True\n",
        "        if hasattr(model.visual, 'config'):\n",
        "            vision_config = model.visual.config\n",
        "            arch_info['vision_layers'] = getattr(vision_config, 'num_hidden_layers', None)\n",
        "            arch_info['vision_hidden_size'] = getattr(vision_config, 'hidden_size', None)\n",
        "    else:\n",
        "        arch_info['has_vision_tower'] = False\n",
        "\n",
        "    return arch_info\n",
        "\n",
        "def get_layer_indices(total_layers):\n",
        "    \"\"\"Get layer indices: 0, n//4, n//2, 3n//4, n-1\"\"\"\n",
        "    if total_layers < 5:\n",
        "        return list(range(total_layers))\n",
        "\n",
        "    indices = [\n",
        "        0,                          # First layer\n",
        "        total_layers // 4,          # n//4\n",
        "        total_layers // 2,          # n//2\n",
        "        (3 * total_layers) // 4,    # 3n//4\n",
        "        total_layers - 1            # Last layer (n-1)\n",
        "    ]\n",
        "\n",
        "    # Remove duplicates and sort\n",
        "    return sorted(list(set(indices)))\n",
        "\n",
        "def detect_and_configure_layers(model):\n",
        "    \"\"\"Detect model architecture and configure layer selection.\"\"\"\n",
        "    print(\"🔍 Detecting Qwen2.5-VL model architecture...\")\n",
        "\n",
        "    arch_info = get_model_architecture_info(model)\n",
        "    print(f\"Architecture info: {arch_info}\")\n",
        "\n",
        "    # Get total language model layers\n",
        "    total_layers = arch_info.get('language_layers')\n",
        "\n",
        "    if total_layers is None:\n",
        "        # Default based on model size\n",
        "        if \"3B\" in MODEL_NAME:\n",
        "            total_layers = 36\n",
        "        else:\n",
        "            total_layers = 28  # For 7B model\n",
        "        print(f\"Could not determine layer count. Using default: {total_layers}\")\n",
        "\n",
        "    # Get target layer indices\n",
        "    selected_layers = get_layer_indices(total_layers)\n",
        "\n",
        "    config = {\n",
        "        'model_architecture': arch_info,\n",
        "        'total_layers': total_layers,\n",
        "        'selected_layers': selected_layers,\n",
        "        'has_vision_tower': arch_info.get('has_vision_tower', False),\n",
        "        'model_name': MODEL_NAME\n",
        "    }\n",
        "\n",
        "    print(f\"📋 Configuration:\")\n",
        "    print(f\"   Total language layers: {total_layers}\")\n",
        "    print(f\"   Selected layers: {selected_layers}\")\n",
        "    print(f\"   Has vision tower: {config['has_vision_tower']}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "def load_image_ids():\n",
        "    \"\"\"Load image IDs from the directory.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(IMAGE_DIR):\n",
        "            print(f\"Error: Image directory {IMAGE_DIR} not found\")\n",
        "            return []\n",
        "\n",
        "        image_files = [f for f in os.listdir(IMAGE_DIR) if f.endswith('.jpg')]\n",
        "        image_ids = sorted([os.path.splitext(f)[0] for f in image_files])\n",
        "\n",
        "        if TEST_MODE:\n",
        "            image_ids = image_ids[:2]\n",
        "            print(f\"🧪 TEST MODE: Processing only {len(image_ids)} images\")\n",
        "\n",
        "        print(f\"Found {len(image_ids)} images in {IMAGE_DIR}\")\n",
        "        return image_ids\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image IDs: {e}\")\n",
        "        return []\n",
        "\n",
        "def safe_tensor_to_numpy(tensor, name=\"tensor\"):\n",
        "    \"\"\"Safely convert tensor to numpy array, handling BFloat16 and other dtypes.\"\"\"\n",
        "    try:\n",
        "        tensor_cpu = tensor.cpu()\n",
        "\n",
        "        # Convert BFloat16 to Float32 if needed\n",
        "        if tensor_cpu.dtype == torch.bfloat16:\n",
        "            tensor_cpu = tensor_cpu.to(torch.float32)\n",
        "        elif tensor_cpu.dtype == torch.float16:\n",
        "            tensor_cpu = tensor_cpu.to(torch.float32)\n",
        "\n",
        "        return tensor_cpu.numpy().tolist()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {name} to numpy: {e}\")\n",
        "        return torch.zeros_like(tensor).float().cpu().numpy().tolist()\n",
        "\n",
        "def find_token_positions(input_ids, processor, vision_token_count):\n",
        "    \"\"\"Find the positions of image end and query end tokens.\"\"\"\n",
        "    token_ids = input_ids[0].cpu().tolist()\n",
        "    sequence_length = len(token_ids)\n",
        "\n",
        "    # Image tokens are at the beginning, so image_end_pos is after vision tokens\n",
        "    image_end_pos = min(vision_token_count, sequence_length - 10)\n",
        "\n",
        "    # Query end is the last token before generation\n",
        "    query_end_pos = sequence_length - 1\n",
        "\n",
        "    # Ensure positions are valid\n",
        "    image_end_pos = max(1, min(image_end_pos, sequence_length - 2))\n",
        "    query_end_pos = max(image_end_pos + 1, min(query_end_pos, sequence_length - 1))\n",
        "\n",
        "    return image_end_pos, query_end_pos\n",
        "\n",
        "def extract_vision_embeddings_from_sequence(hidden_states, vision_token_count):\n",
        "    \"\"\"Extract vision embeddings from the sequence hidden states.\"\"\"\n",
        "    try:\n",
        "        # Vision tokens are typically at the beginning of the sequence\n",
        "        # We'll extract from the last layer and pool the vision region\n",
        "\n",
        "        final_layer = hidden_states[-1]  # Last layer hidden states\n",
        "\n",
        "        # Extract vision tokens (first vision_token_count tokens)\n",
        "        vision_tokens = final_layer[0, :vision_token_count, :]  # [vision_tokens, hidden_size]\n",
        "\n",
        "        # Pool across vision tokens to get a single representation\n",
        "        vision_embedding = vision_tokens.mean(dim=0)  # [hidden_size]\n",
        "\n",
        "        return safe_tensor_to_numpy(vision_embedding, \"vision_embeddings_from_sequence\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not extract vision embeddings from sequence: {e}\")\n",
        "        return None\n",
        "\n",
        "def estimate_vision_token_count(input_ids, sequence_length):\n",
        "    \"\"\"Estimate the number of vision tokens in the sequence.\"\"\"\n",
        "    # For Qwen2.5-VL, vision tokens are typically at the beginning\n",
        "    # Common ranges are 256-1024 tokens depending on image resolution\n",
        "\n",
        "    # Conservative estimate: vision tokens are roughly 10-30% of sequence\n",
        "    # but usually between 200-800 tokens\n",
        "    estimated_vision_tokens = min(\n",
        "        max(200, sequence_length // 4),  # At least 200, at most 1/4 of sequence\n",
        "        800  # Cap at 800 tokens\n",
        "    )\n",
        "\n",
        "    return estimated_vision_tokens\n",
        "\n",
        "def process_image_with_qwen2_5vl(image_path):\n",
        "    \"\"\"Process image with Qwen2.5-VL processor.\"\"\"\n",
        "    global processor, model\n",
        "\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Create messages\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"image\",\n",
        "                        \"image\": image,\n",
        "                    },\n",
        "                    {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Process following reference code\n",
        "        text = processor.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "        inputs = processor(\n",
        "            text=[text],\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        inputs = inputs.to(model.device)\n",
        "\n",
        "        return inputs, text, messages\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def extract_targeted_embeddings(image_id, image_path, selected_layers):\n",
        "    \"\"\"Extract only the specific embeddings we need.\"\"\"\n",
        "    global model, processor, device\n",
        "\n",
        "    try:\n",
        "        # Process image\n",
        "        inputs, text, messages = process_image_with_qwen2_5vl(image_path)\n",
        "        if inputs is None:\n",
        "            return None\n",
        "\n",
        "        embeddings_data = {}\n",
        "\n",
        "        # Forward pass to get all hidden states\n",
        "        with torch.no_grad():\n",
        "            model_outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "            if hasattr(model_outputs, 'hidden_states') and model_outputs.hidden_states:\n",
        "                hidden_states = model_outputs.hidden_states\n",
        "                total_available_layers = len(hidden_states)\n",
        "\n",
        "                # Get sequence info\n",
        "                sequence_length = len(inputs['input_ids'][0])\n",
        "\n",
        "                # Estimate vision token count\n",
        "                vision_token_count = estimate_vision_token_count(inputs['input_ids'], sequence_length)\n",
        "\n",
        "                # 1. Extract Vision Embeddings from the sequence\n",
        "                vision_embeddings = extract_vision_embeddings_from_sequence(hidden_states, vision_token_count)\n",
        "                embeddings_data['vision_embeddings'] = vision_embeddings\n",
        "\n",
        "                # 2. Extract Pre-generation Embeddings at Specific Layers and Positions\n",
        "                # Validate selected layers\n",
        "                valid_layers = [layer for layer in selected_layers if layer < total_available_layers]\n",
        "                if len(valid_layers) != len(selected_layers):\n",
        "                    invalid_layers = [layer for layer in selected_layers if layer >= total_available_layers]\n",
        "                    print(f\"   ⚠️ Skipping invalid layers {invalid_layers} (max: {total_available_layers-1})\")\n",
        "\n",
        "                # Find critical token positions\n",
        "                image_end_pos, query_end_pos = find_token_positions(inputs['input_ids'], processor, vision_token_count)\n",
        "\n",
        "                # Extract embeddings from selected layers at critical positions\n",
        "                pre_generation_data = {}\n",
        "\n",
        "                for layer_idx in valid_layers:\n",
        "                    layer_hidden = hidden_states[layer_idx]\n",
        "\n",
        "                    # Extract embeddings at the two critical positions\n",
        "                    after_image_emb = layer_hidden[0, image_end_pos, :]\n",
        "                    end_query_emb = layer_hidden[0, query_end_pos, :]\n",
        "\n",
        "                    pre_generation_data[f'layer_{layer_idx}'] = {\n",
        "                        'after_image_embeddings': safe_tensor_to_numpy(after_image_emb, f\"layer_{layer_idx}_after_image\"),\n",
        "                        'end_query_embeddings': safe_tensor_to_numpy(end_query_emb, f\"layer_{layer_idx}_end_query\")\n",
        "                    }\n",
        "\n",
        "                embeddings_data['pre_generation'] = pre_generation_data\n",
        "\n",
        "                # Store position info for reference\n",
        "                embeddings_data['token_positions'] = {\n",
        "                    'image_end_position': int(image_end_pos),\n",
        "                    'query_end_position': int(query_end_pos),\n",
        "                    'sequence_length': int(sequence_length),\n",
        "                    'estimated_vision_tokens': int(vision_token_count)\n",
        "                }\n",
        "\n",
        "        # Generate caption for reference\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "            generated_ids_trimmed = [\n",
        "                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "            ]\n",
        "            output_text = processor.batch_decode(\n",
        "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "            )\n",
        "            generated_caption = output_text[0] if output_text else \"\"\n",
        "\n",
        "        # Clear GPU memory\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return {\n",
        "            'image_id': image_id,\n",
        "            'generated_caption': generated_caption,\n",
        "            'vision_embeddings': embeddings_data.get('vision_embeddings'),\n",
        "            'pre_generation': embeddings_data.get('pre_generation', {}),\n",
        "            'token_positions': embeddings_data.get('token_positions', {})\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting embeddings for {image_id}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def write_embeddings_hdf5(f, image_id, result):\n",
        "    \"\"\"Write targeted embeddings to HDF5 file.\"\"\"\n",
        "    try:\n",
        "        group = f.create_group(image_id)\n",
        "\n",
        "        # Basic metadata\n",
        "        group.create_dataset('image_id', data=result['image_id'].encode('utf-8'))\n",
        "        group.create_dataset('generated_caption', data=result['generated_caption'].encode('utf-8'))\n",
        "\n",
        "        # Token positions info\n",
        "        if result['token_positions']:\n",
        "            pos_group = group.create_group('token_positions')\n",
        "            for key, value in result['token_positions'].items():\n",
        "                pos_group.create_dataset(key, data=value)\n",
        "\n",
        "        # Vision embeddings\n",
        "        if result['vision_embeddings'] is not None:\n",
        "            vision_emb = np.array(result['vision_embeddings'], dtype=np.float32)\n",
        "            group.create_dataset('vision_embeddings',\n",
        "                               data=vision_emb,\n",
        "                               chunks=True,\n",
        "                               compression='gzip')\n",
        "\n",
        "        # Pre-generation embeddings\n",
        "        pre_gen_group = group.create_group('pre_generation')\n",
        "        for layer_name, layer_data in result['pre_generation'].items():\n",
        "            layer_group = pre_gen_group.create_group(layer_name)\n",
        "\n",
        "            # After image embeddings\n",
        "            if 'after_image_embeddings' in layer_data:\n",
        "                layer_group.create_dataset('after_image_embeddings',\n",
        "                                         data=np.array(layer_data['after_image_embeddings'], dtype=np.float32),\n",
        "                                         chunks=True,\n",
        "                                         compression='gzip')\n",
        "\n",
        "            # End query embeddings\n",
        "            if 'end_query_embeddings' in layer_data:\n",
        "                layer_group.create_dataset('end_query_embeddings',\n",
        "                                         data=np.array(layer_data['end_query_embeddings'], dtype=np.float32),\n",
        "                                         chunks=True,\n",
        "                                         compression='gzip')\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing embeddings for image {image_id}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_ids, image_dir):\n",
        "        self.image_ids = image_ids\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        image_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n",
        "\n",
        "        if os.path.exists(image_path):\n",
        "            return image_id, image_path\n",
        "        else:\n",
        "            return image_id, None\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader.\"\"\"\n",
        "    valid_batch = [(img_id, img_path) for img_id, img_path in batch if img_path is not None]\n",
        "\n",
        "    if not valid_batch:\n",
        "        return [], []\n",
        "\n",
        "    image_ids = [item[0] for item in valid_batch]\n",
        "    image_paths = [item[1] for item in valid_batch]\n",
        "\n",
        "    return image_ids, image_paths\n",
        "\n",
        "def process_batch(batch_data, selected_layers):\n",
        "    \"\"\"Process a batch of images.\"\"\"\n",
        "    image_ids, image_paths = batch_data\n",
        "    results = []\n",
        "\n",
        "    if not image_ids or not image_paths:\n",
        "        return results\n",
        "\n",
        "    for image_id, image_path in zip(image_ids, image_paths):\n",
        "        try:\n",
        "            result = extract_targeted_embeddings(image_id, image_path, selected_layers)\n",
        "            if result:\n",
        "                results.append((image_id, result))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with GPU usage and progress tracking.\"\"\"\n",
        "    global processor, model, device\n",
        "\n",
        "    try:\n",
        "        print(\"🚀 Starting Qwen2.5-VL Targeted Embeddings Extraction\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"📊 Settings: {PART_SIZE} images per H5 file\")\n",
        "        if TEST_MODE:\n",
        "            print(\"🧪 RUNNING IN TEST MODE - Processing only 2 images\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Check GPU and load model\n",
        "        device = check_gpu_availability()\n",
        "\n",
        "        print(\"📥 Loading Qwen2.5-VL model and processor...\")\n",
        "        print(f\"Model: {MODEL_NAME}\")\n",
        "\n",
        "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        device = model.device\n",
        "        model_dtype = next(model.parameters()).dtype\n",
        "        print(f\"🖥️  Model loaded on device: {device}\")\n",
        "        print(f\"🔢 Model dtype: {model_dtype}\")\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Configure layers\n",
        "        config = detect_and_configure_layers(model)\n",
        "        selected_layers = config['selected_layers']\n",
        "\n",
        "        # Load image IDs\n",
        "        image_ids = load_image_ids()\n",
        "        if not image_ids:\n",
        "            print(\"No image IDs found. Exiting.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found {len(image_ids)} image IDs\")\n",
        "\n",
        "        if TEST_MODE:\n",
        "            PART_SIZE_ACTUAL = min(PART_SIZE, len(image_ids))\n",
        "        else:\n",
        "            PART_SIZE_ACTUAL = PART_SIZE\n",
        "\n",
        "        # Process in parts\n",
        "        total_parts = (len(image_ids) + PART_SIZE_ACTUAL - 1) // PART_SIZE_ACTUAL\n",
        "\n",
        "        if TEST_MODE:\n",
        "            print(f\"🧪 TEST MODE: Processing {len(image_ids)} images in {total_parts} part(s)\")\n",
        "\n",
        "        # Overall progress bar\n",
        "        overall_pbar = tqdm(total=len(image_ids), desc=\"Overall Progress\", position=0)\n",
        "\n",
        "        for part in range(total_parts):\n",
        "            start_idx = part * PART_SIZE_ACTUAL\n",
        "            end_idx = min((part + 1) * PART_SIZE_ACTUAL, len(image_ids))\n",
        "            current_batch = image_ids[start_idx:end_idx]\n",
        "\n",
        "            # Create output directory\n",
        "            os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "            # Output file\n",
        "            current_output_path = os.path.join(OUTPUT_DIR, f\"qwen2_5_vl_targeted_embeddings_part{part+1}.h5\")\n",
        "\n",
        "            print(f\"\\n📦 Processing part {part+1}/{total_parts}\")\n",
        "            print(f\"🖼️  Processing images {start_idx+1} to {end_idx}\")\n",
        "            print(f\"💾 Output file: {current_output_path}\")\n",
        "\n",
        "            # Create dataset and dataloader\n",
        "            dataset = ImageDataset(current_batch, IMAGE_DIR)\n",
        "            dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                num_workers=NUM_WORKERS,\n",
        "                shuffle=False,\n",
        "                collate_fn=collate_fn\n",
        "            )\n",
        "\n",
        "            # Process images with progress bar\n",
        "            part_pbar = tqdm(total=len(current_batch), desc=f\"Part {part+1}\", position=1, leave=False)\n",
        "\n",
        "            with h5py.File(current_output_path, 'w') as f:\n",
        "                # Store configuration\n",
        "                f.attrs['model_name'] = MODEL_NAME\n",
        "                f.attrs['model_config'] = json.dumps(config, default=str)\n",
        "                f.attrs['extraction_type'] = 'targeted_embeddings_fixed'\n",
        "                f.attrs['selected_layers'] = json.dumps(selected_layers)\n",
        "\n",
        "                saved_count = 0\n",
        "\n",
        "                for batch_idx, batch_data in enumerate(dataloader):\n",
        "                    try:\n",
        "                        batch_results = process_batch(batch_data, selected_layers)\n",
        "\n",
        "                        for image_id, result in batch_results:\n",
        "                            try:\n",
        "                                if write_embeddings_hdf5(f, image_id, result):\n",
        "                                    saved_count += 1\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error saving {image_id}: {e}\")\n",
        "\n",
        "                        # Update progress bars\n",
        "                        batch_size = len(batch_data[0]) if batch_data[0] else 0\n",
        "                        part_pbar.update(batch_size)\n",
        "                        overall_pbar.update(batch_size)\n",
        "\n",
        "                        # GPU memory cleanup\n",
        "                        if torch.cuda.is_available() and batch_idx % 10 == 0:\n",
        "                            torch.cuda.empty_cache()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing batch {batch_idx + 1}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            part_pbar.close()\n",
        "\n",
        "            print(f\"✅ Part {part+1} completed: {saved_count} images saved\")\n",
        "\n",
        "            if TEST_MODE:\n",
        "                print(\"🧪 TEST MODE: Successfully processed test images!\")\n",
        "                break\n",
        "\n",
        "            # Memory cleanup between parts\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        overall_pbar.close()\n",
        "\n",
        "        if TEST_MODE:\n",
        "            print(\"\\n🎉 TEST MODE COMPLETED SUCCESSFULLY!\")\n",
        "            print(\"✅ Targeted embeddings extracted without errors\")\n",
        "            print(\"✅ Ready for full dataset processing (set TEST_MODE = False)\")\n",
        "        else:\n",
        "            print(\"\\n🎉 All parts processing complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in main function: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        mp.set_start_method('spawn', force=True)\n",
        "    except RuntimeError:\n",
        "        pass\n",
        "\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import traceback\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.multiprocessing as mp\n",
        "import warnings\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def check_gpu_availability():\n",
        "    \"\"\"Check GPU availability and set appropriate device.\"\"\"\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            device_count = torch.cuda.device_count()\n",
        "            print(f\"CUDA is available with {device_count} GPU(s)\")\n",
        "\n",
        "            # Try to create a tensor on GPU to test functionality\n",
        "            test_tensor = torch.tensor([1.0]).cuda()\n",
        "            del test_tensor\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            return \"cuda\"\n",
        "        else:\n",
        "            print(\"CUDA is not available\")\n",
        "            return \"cpu\"\n",
        "    except Exception as e:\n",
        "        print(f\"GPU test failed: {e}\")\n",
        "        print(\"Falling back to CPU\")\n",
        "        return \"cpu\"\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 1  # Qwen2.5-VL requires batch size 1 for vision processing\n",
        "PART_SIZE = 250  # Images per part file (as requested)\n",
        "NUM_WORKERS = 0  # Disabled multiprocessing for Qwen2.5-VL\n",
        "IMAGE_DIR = \"/root/akhil_workspace/coco_val2014\"\n",
        "OUTPUT_DIR = \"/root/akhil_workspace/qwen_model/extracted_embeddings_coco2014\"\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"  # Can also use \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "# Test mode - set to True to process only 2 images for testing\n",
        "TEST_MODE = False  # Set to False for full processing\n",
        "\n",
        "# Global model variables (will be initialized in main)\n",
        "processor = None\n",
        "model = None\n",
        "device = None\n",
        "\n",
        "def get_model_architecture_info(model):\n",
        "    \"\"\"Get the total number of layers in the language model.\"\"\"\n",
        "    arch_info = {}\n",
        "\n",
        "    # Get language model layers\n",
        "    if hasattr(model, 'model') and hasattr(model.model, 'config'):\n",
        "        config = model.model.config\n",
        "        arch_info['language_layers'] = getattr(config, 'num_hidden_layers', None)\n",
        "        arch_info['language_hidden_size'] = getattr(config, 'hidden_size', None)\n",
        "\n",
        "    # Get vision model info\n",
        "    if hasattr(model, 'visual'):\n",
        "        arch_info['has_vision_tower'] = True\n",
        "        if hasattr(model.visual, 'config'):\n",
        "            vision_config = model.visual.config\n",
        "            arch_info['vision_layers'] = getattr(vision_config, 'num_hidden_layers', None)\n",
        "            arch_info['vision_hidden_size'] = getattr(vision_config, 'hidden_size', None)\n",
        "    else:\n",
        "        arch_info['has_vision_tower'] = False\n",
        "\n",
        "    return arch_info\n",
        "\n",
        "def get_layer_indices(total_layers):\n",
        "    \"\"\"Get layer indices: 0, n//4, n//2, 3n//4, n-1\"\"\"\n",
        "    if total_layers < 5:\n",
        "        return list(range(total_layers))\n",
        "\n",
        "    indices = [\n",
        "        0,                          # First layer\n",
        "        total_layers // 4,          # n//4\n",
        "        total_layers // 2,          # n//2\n",
        "        (3 * total_layers) // 4,    # 3n//4\n",
        "        total_layers - 1            # Last layer (n-1)\n",
        "    ]\n",
        "\n",
        "    # Remove duplicates and sort\n",
        "    return sorted(list(set(indices)))\n",
        "\n",
        "def detect_and_configure_layers(model):\n",
        "    \"\"\"Detect model architecture and configure layer selection.\"\"\"\n",
        "    print(\"🔍 Detecting Qwen2.5-VL model architecture...\")\n",
        "\n",
        "    arch_info = get_model_architecture_info(model)\n",
        "    print(f\"Architecture info: {arch_info}\")\n",
        "\n",
        "    # Get total language model layers\n",
        "    total_layers = arch_info.get('language_layers')\n",
        "\n",
        "    if total_layers is None:\n",
        "        # Default based on model size\n",
        "        if \"3B\" in MODEL_NAME:\n",
        "            total_layers = 36\n",
        "        else:\n",
        "            total_layers = 28  # For 7B model\n",
        "        print(f\"Could not determine layer count. Using default: {total_layers}\")\n",
        "\n",
        "    # Get target layer indices\n",
        "    selected_layers = get_layer_indices(total_layers)\n",
        "\n",
        "    config = {\n",
        "        'model_architecture': arch_info,\n",
        "        'total_layers': total_layers,\n",
        "        'selected_layers': selected_layers,\n",
        "        'has_vision_tower': arch_info.get('has_vision_tower', False),\n",
        "        'model_name': MODEL_NAME\n",
        "    }\n",
        "\n",
        "    print(f\"📋 Configuration:\")\n",
        "    print(f\"   Total language layers: {total_layers}\")\n",
        "    print(f\"   Selected layers: {selected_layers}\")\n",
        "    print(f\"   Has vision tower: {config['has_vision_tower']}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "def load_image_ids():\n",
        "    \"\"\"Load image IDs from the directory.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(IMAGE_DIR):\n",
        "            print(f\"Error: Image directory {IMAGE_DIR} not found\")\n",
        "            return []\n",
        "\n",
        "        image_files = [f for f in os.listdir(IMAGE_DIR) if f.endswith('.jpg')]\n",
        "        image_ids = sorted([os.path.splitext(f)[0] for f in image_files])\n",
        "\n",
        "        if TEST_MODE:\n",
        "            image_ids = image_ids[:2]\n",
        "            print(f\"🧪 TEST MODE: Processing only {len(image_ids)} images\")\n",
        "\n",
        "        print(f\"Found {len(image_ids)} images in {IMAGE_DIR}\")\n",
        "        return image_ids\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image IDs: {e}\")\n",
        "        return []\n",
        "\n",
        "def safe_tensor_to_numpy(tensor, name=\"tensor\"):\n",
        "    \"\"\"Efficiently convert tensor to numpy array, keeping operations on GPU until final step.\"\"\"\n",
        "    try:\n",
        "        # Keep tensor on GPU for as long as possible, only move to CPU at the end\n",
        "        if tensor.dtype == torch.bfloat16:\n",
        "            tensor = tensor.to(torch.float32)\n",
        "        elif tensor.dtype == torch.float16:\n",
        "            tensor = tensor.to(torch.float32)\n",
        "\n",
        "        # Single CPU transfer at the end\n",
        "        return tensor.cpu().numpy().tolist()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {name} to numpy: {e}\")\n",
        "        return torch.zeros_like(tensor).float().cpu().numpy().tolist()\n",
        "\n",
        "def find_token_positions(input_ids, processor, vision_token_count):\n",
        "    \"\"\"Find the positions of image end and query end tokens.\"\"\"\n",
        "    token_ids = input_ids[0].cpu().tolist()\n",
        "    sequence_length = len(token_ids)\n",
        "\n",
        "    # Image tokens are at the beginning, so image_end_pos is after vision tokens\n",
        "    image_end_pos = min(vision_token_count, sequence_length - 10)\n",
        "\n",
        "    # Query end is the last token before generation\n",
        "    query_end_pos = sequence_length - 1\n",
        "\n",
        "    # Ensure positions are valid\n",
        "    image_end_pos = max(1, min(image_end_pos, sequence_length - 2))\n",
        "    query_end_pos = max(image_end_pos + 1, min(query_end_pos, sequence_length - 1))\n",
        "\n",
        "    return image_end_pos, query_end_pos\n",
        "\n",
        "def extract_vision_embeddings_from_sequence(hidden_states, vision_token_count):\n",
        "    \"\"\"Extract vision embeddings from the sequence hidden states - GPU optimized.\"\"\"\n",
        "    try:\n",
        "        # Keep everything on GPU until final conversion\n",
        "        final_layer = hidden_states[-1]  # Already on GPU\n",
        "\n",
        "        # Extract vision tokens (first vision_token_count tokens) - GPU operation\n",
        "        vision_tokens = final_layer[0, :vision_token_count, :]  # [vision_tokens, hidden_size]\n",
        "\n",
        "        # Pool across vision tokens - GPU operation\n",
        "        vision_embedding = vision_tokens.mean(dim=0)  # [hidden_size]\n",
        "\n",
        "        return safe_tensor_to_numpy(vision_embedding, \"vision_embeddings_from_sequence\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not extract vision embeddings from sequence: {e}\")\n",
        "        return None\n",
        "\n",
        "def estimate_vision_token_count(input_ids, sequence_length):\n",
        "    \"\"\"Estimate the number of vision tokens in the sequence.\"\"\"\n",
        "    # For Qwen2.5-VL, vision tokens are typically at the beginning\n",
        "    # Common ranges are 256-1024 tokens depending on image resolution\n",
        "\n",
        "    # Conservative estimate: vision tokens are roughly 10-30% of sequence\n",
        "    # but usually between 200-800 tokens\n",
        "    estimated_vision_tokens = min(\n",
        "        max(200, sequence_length // 4),  # At least 200, at most 1/4 of sequence\n",
        "        800  # Cap at 800 tokens\n",
        "    )\n",
        "\n",
        "    return estimated_vision_tokens\n",
        "\n",
        "def process_image_with_qwen2_5vl(image_path):\n",
        "    \"\"\"Process image with Qwen2.5-VL processor.\"\"\"\n",
        "    global processor, model\n",
        "\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Create messages\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"image\",\n",
        "                        \"image\": image,\n",
        "                    },\n",
        "                    {\"type\": \"text\", \"text\": \"Caption this image.\"},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Process following reference code\n",
        "        text = processor.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "        inputs = processor(\n",
        "            text=[text],\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        inputs = inputs.to(model.device)\n",
        "\n",
        "        return inputs, text, messages\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def extract_targeted_embeddings(image_id, image_path, selected_layers):\n",
        "    \"\"\"Extract only the specific embeddings we need - GPU optimized.\"\"\"\n",
        "    global model, processor, device\n",
        "\n",
        "    try:\n",
        "        # Process image\n",
        "        inputs, text, messages = process_image_with_qwen2_5vl(image_path)\n",
        "        if inputs is None:\n",
        "            return None\n",
        "\n",
        "        embeddings_data = {}\n",
        "\n",
        "        # Single forward pass to get all hidden states - keep on GPU\n",
        "        with torch.no_grad():\n",
        "            # Use torch.cuda.amp for mixed precision if available\n",
        "            if device == \"cuda\":\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    model_outputs = model(**inputs, output_hidden_states=True)\n",
        "            else:\n",
        "                model_outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "            if hasattr(model_outputs, 'hidden_states') and model_outputs.hidden_states:\n",
        "                hidden_states = model_outputs.hidden_states\n",
        "                total_available_layers = len(hidden_states)\n",
        "\n",
        "                # Get sequence info - minimal CPU operations\n",
        "                sequence_length = inputs['input_ids'].shape[1]  # Keep tensor operation on GPU\n",
        "\n",
        "                # Estimate vision token count\n",
        "                vision_token_count = estimate_vision_token_count(inputs['input_ids'], sequence_length)\n",
        "\n",
        "                # 1. Extract Vision Embeddings from the sequence\n",
        "                vision_embeddings = extract_vision_embeddings_from_sequence(hidden_states, vision_token_count)\n",
        "                embeddings_data['vision_embeddings'] = vision_embeddings\n",
        "\n",
        "                # 2. Extract Pre-generation Embeddings at Specific Layers and Positions\n",
        "                # Validate selected layers\n",
        "                valid_layers = [layer for layer in selected_layers if layer < total_available_layers]\n",
        "\n",
        "                # Find critical token positions\n",
        "                image_end_pos, query_end_pos = find_token_positions(inputs['input_ids'], processor, vision_token_count)\n",
        "\n",
        "                # Batch extract embeddings from all layers - GPU optimized\n",
        "                pre_generation_data = {}\n",
        "\n",
        "                # Extract from multiple layers in one go to reduce GPU operations\n",
        "                for layer_idx in valid_layers:\n",
        "                    layer_hidden = hidden_states[layer_idx]  # Already on GPU\n",
        "\n",
        "                    # Extract both positions at once - GPU operations\n",
        "                    after_image_emb = layer_hidden[0, image_end_pos, :]\n",
        "                    end_query_emb = layer_hidden[0, query_end_pos, :]\n",
        "\n",
        "                    # Convert to numpy only at the end\n",
        "                    pre_generation_data[f'layer_{layer_idx}'] = {\n",
        "                        'after_image_embeddings': safe_tensor_to_numpy(after_image_emb, f\"layer_{layer_idx}_after_image\"),\n",
        "                        'end_query_embeddings': safe_tensor_to_numpy(end_query_emb, f\"layer_{layer_idx}_end_query\")\n",
        "                    }\n",
        "\n",
        "                embeddings_data['pre_generation'] = pre_generation_data\n",
        "\n",
        "                # Store position info\n",
        "                embeddings_data['token_positions'] = {\n",
        "                    'image_end_position': int(image_end_pos),\n",
        "                    'query_end_position': int(query_end_pos),\n",
        "                    'sequence_length': int(sequence_length),\n",
        "                    'estimated_vision_tokens': int(vision_token_count)\n",
        "                }\n",
        "\n",
        "        # Generate caption efficiently - separate forward pass\n",
        "        with torch.no_grad():\n",
        "            if device == \"cuda\":\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    generated_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False)  # Reduced tokens for speed\n",
        "            else:\n",
        "                generated_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
        "\n",
        "            generated_ids_trimmed = [\n",
        "                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "            ]\n",
        "            output_text = processor.batch_decode(\n",
        "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "            )\n",
        "            generated_caption = output_text[0] if output_text else \"\"\n",
        "\n",
        "        return {\n",
        "            'image_id': image_id,\n",
        "            'generated_caption': generated_caption,\n",
        "            'vision_embeddings': embeddings_data.get('vision_embeddings'),\n",
        "            'pre_generation': embeddings_data.get('pre_generation', {}),\n",
        "            'token_positions': embeddings_data.get('token_positions', {})\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting embeddings for {image_id}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def write_embeddings_hdf5(f, image_id, result):\n",
        "    \"\"\"Write targeted embeddings to HDF5 file.\"\"\"\n",
        "    try:\n",
        "        group = f.create_group(image_id)\n",
        "\n",
        "        # Basic metadata\n",
        "        group.create_dataset('image_id', data=result['image_id'].encode('utf-8'))\n",
        "        group.create_dataset('generated_caption', data=result['generated_caption'].encode('utf-8'))\n",
        "\n",
        "        # Token positions info\n",
        "        if result['token_positions']:\n",
        "            pos_group = group.create_group('token_positions')\n",
        "            for key, value in result['token_positions'].items():\n",
        "                pos_group.create_dataset(key, data=value)\n",
        "\n",
        "        # Vision embeddings\n",
        "        if result['vision_embeddings'] is not None:\n",
        "            vision_emb = np.array(result['vision_embeddings'], dtype=np.float32)\n",
        "            group.create_dataset('vision_embeddings',\n",
        "                               data=vision_emb,\n",
        "                               chunks=True,\n",
        "                               compression='gzip')\n",
        "\n",
        "        # Pre-generation embeddings\n",
        "        pre_gen_group = group.create_group('pre_generation')\n",
        "        for layer_name, layer_data in result['pre_generation'].items():\n",
        "            layer_group = pre_gen_group.create_group(layer_name)\n",
        "\n",
        "            # After image embeddings\n",
        "            if 'after_image_embeddings' in layer_data:\n",
        "                layer_group.create_dataset('after_image_embeddings',\n",
        "                                         data=np.array(layer_data['after_image_embeddings'], dtype=np.float32),\n",
        "                                         chunks=True,\n",
        "                                         compression='gzip')\n",
        "\n",
        "            # End query embeddings\n",
        "            if 'end_query_embeddings' in layer_data:\n",
        "                layer_group.create_dataset('end_query_embeddings',\n",
        "                                         data=np.array(layer_data['end_query_embeddings'], dtype=np.float32),\n",
        "                                         chunks=True,\n",
        "                                         compression='gzip')\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing embeddings for image {image_id}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_ids, image_dir):\n",
        "        self.image_ids = image_ids\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        image_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n",
        "\n",
        "        if os.path.exists(image_path):\n",
        "            return image_id, image_path\n",
        "        else:\n",
        "            return image_id, None\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader.\"\"\"\n",
        "    valid_batch = [(img_id, img_path) for img_id, img_path in batch if img_path is not None]\n",
        "\n",
        "    if not valid_batch:\n",
        "        return [], []\n",
        "\n",
        "    image_ids = [item[0] for item in valid_batch]\n",
        "    image_paths = [item[1] for item in valid_batch]\n",
        "\n",
        "    return image_ids, image_paths\n",
        "\n",
        "def process_batch(batch_data, selected_layers):\n",
        "    \"\"\"Process a batch of images.\"\"\"\n",
        "    image_ids, image_paths = batch_data\n",
        "    results = []\n",
        "\n",
        "    if not image_ids or not image_paths:\n",
        "        return results\n",
        "\n",
        "    for image_id, image_path in zip(image_ids, image_paths):\n",
        "        try:\n",
        "            result = extract_targeted_embeddings(image_id, image_path, selected_layers)\n",
        "            if result:\n",
        "                results.append((image_id, result))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with GPU usage and progress tracking.\"\"\"\n",
        "    global processor, model, device\n",
        "\n",
        "    try:\n",
        "        print(\"🚀 Starting Qwen2.5-VL Targeted Embeddings Extraction\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"📊 Settings: {PART_SIZE} images per H5 file\")\n",
        "        if TEST_MODE:\n",
        "            print(\"🧪 RUNNING IN TEST MODE - Processing only 2 images\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Check GPU and load model\n",
        "        device = check_gpu_availability()\n",
        "\n",
        "        print(\"📥 Loading Qwen2.5-VL model and processor...\")\n",
        "        print(f\"Model: {MODEL_NAME}\")\n",
        "\n",
        "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        device = model.device\n",
        "        model_dtype = next(model.parameters()).dtype\n",
        "        print(f\"🖥️  Model loaded on device: {device}\")\n",
        "        print(f\"🔢 Model dtype: {model_dtype}\")\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Configure layers\n",
        "        config = detect_and_configure_layers(model)\n",
        "        selected_layers = config['selected_layers']\n",
        "\n",
        "        # Load image IDs\n",
        "        image_ids = load_image_ids()\n",
        "        if not image_ids:\n",
        "            print(\"No image IDs found. Exiting.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found {len(image_ids)} image IDs\")\n",
        "\n",
        "        if TEST_MODE:\n",
        "            PART_SIZE_ACTUAL = min(PART_SIZE, len(image_ids))\n",
        "        else:\n",
        "            PART_SIZE_ACTUAL = PART_SIZE\n",
        "\n",
        "        # Process in parts\n",
        "        total_parts = (len(image_ids) + PART_SIZE_ACTUAL - 1) // PART_SIZE_ACTUAL\n",
        "\n",
        "        if TEST_MODE:\n",
        "            print(f\"🧪 TEST MODE: Processing {len(image_ids)} images in {total_parts} part(s)\")\n",
        "\n",
        "        # Overall progress bar\n",
        "        overall_pbar = tqdm(total=len(image_ids), desc=\"Overall Progress\", position=0)\n",
        "\n",
        "        for part in range(total_parts):\n",
        "            start_idx = part * PART_SIZE_ACTUAL\n",
        "            end_idx = min((part + 1) * PART_SIZE_ACTUAL, len(image_ids))\n",
        "            current_batch = image_ids[start_idx:end_idx]\n",
        "\n",
        "            # Create output directory\n",
        "            os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "            # Output file\n",
        "            current_output_path = os.path.join(OUTPUT_DIR, f\"embeddings_part{part+1}.h5\")\n",
        "\n",
        "            print(f\"\\n📦 Processing part {part+1}/{total_parts}\")\n",
        "            print(f\"🖼️  Processing images {start_idx+1} to {end_idx}\")\n",
        "            print(f\"💾 Output file: {current_output_path}\")\n",
        "\n",
        "            # Create dataset and dataloader\n",
        "            dataset = ImageDataset(current_batch, IMAGE_DIR)\n",
        "            dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                num_workers=NUM_WORKERS,\n",
        "                shuffle=False,\n",
        "                collate_fn=collate_fn\n",
        "            )\n",
        "\n",
        "            # Process images with progress bar\n",
        "            part_pbar = tqdm(total=len(current_batch), desc=f\"Part {part+1}\", position=1, leave=False)\n",
        "\n",
        "            with h5py.File(current_output_path, 'w') as f:\n",
        "                # Store configuration\n",
        "                f.attrs['model_name'] = MODEL_NAME\n",
        "                f.attrs['model_config'] = json.dumps(config, default=str)\n",
        "                f.attrs['extraction_type'] = 'targeted_embeddings_fixed'\n",
        "                f.attrs['selected_layers'] = json.dumps(selected_layers)\n",
        "\n",
        "                saved_count = 0\n",
        "\n",
        "                for batch_idx, batch_data in enumerate(dataloader):\n",
        "                    try:\n",
        "                        batch_results = process_batch(batch_data, selected_layers)\n",
        "\n",
        "                        for image_id, result in batch_results:\n",
        "                            try:\n",
        "                                if write_embeddings_hdf5(f, image_id, result):\n",
        "                                    saved_count += 1\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error saving {image_id}: {e}\")\n",
        "\n",
        "                        # Update progress bars\n",
        "                        batch_size = len(batch_data[0]) if batch_data[0] else 0\n",
        "                        part_pbar.update(batch_size)\n",
        "                        overall_pbar.update(batch_size)\n",
        "\n",
        "                        # GPU memory cleanup\n",
        "                        if torch.cuda.is_available() and batch_idx % 10 == 0:\n",
        "                            torch.cuda.empty_cache()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing batch {batch_idx + 1}: {e}\")\n",
        "                        continue\n",
        "\n",
        "            part_pbar.close()\n",
        "\n",
        "            print(f\"✅ Part {part+1} completed: {saved_count} images saved\")\n",
        "\n",
        "            if TEST_MODE:\n",
        "                print(\"🧪 TEST MODE: Successfully processed test images!\")\n",
        "                break\n",
        "\n",
        "            # Memory cleanup between parts\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        overall_pbar.close()\n",
        "\n",
        "        if TEST_MODE:\n",
        "            print(\"\\n🎉 TEST MODE COMPLETED SUCCESSFULLY!\")\n",
        "            print(\"✅ Targeted embeddings extracted without errors\")\n",
        "            print(\"✅ Ready for full dataset processing (set TEST_MODE = False)\")\n",
        "        else:\n",
        "            print(\"\\n🎉 All parts processing complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in main function: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        mp.set_start_method('spawn', force=True)\n",
        "    except RuntimeError:\n",
        "        pass\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "Bjt8e3HdHdA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "def convert_to_json_safe(value):\n",
        "    \"\"\"Convert any value to JSON-safe format.\"\"\"\n",
        "    if isinstance(value, np.ndarray):\n",
        "        return value.tolist()\n",
        "    elif isinstance(value, bytes):\n",
        "        return value.decode('utf-8')\n",
        "    elif isinstance(value, (np.integer, np.floating)):\n",
        "        return value.item()\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "def extract_embeddings_from_group(group):\n",
        "    \"\"\"Extract all embeddings from H5 group recursively.\"\"\"\n",
        "    result = {}\n",
        "\n",
        "    for key in group.keys():\n",
        "        item = group[key]\n",
        "\n",
        "        if isinstance(item, h5py.Dataset):\n",
        "            # It's data - extract it\n",
        "            data = item[()]\n",
        "            result[key] = convert_to_json_safe(data)\n",
        "        elif isinstance(item, h5py.Group):\n",
        "            # It's a group - go deeper\n",
        "            result[key] = extract_embeddings_from_group(item)\n",
        "\n",
        "    return result\n",
        "\n",
        "def extract_qwen_embeddings_to_json(h5_file_path):\n",
        "    \"\"\"Extract Qwen2.5-VL embeddings from H5 file to JSON.\"\"\"\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(h5_file_path):\n",
        "        print(f\"❌ Error: File not found - {h5_file_path}\")\n",
        "        return\n",
        "\n",
        "    # Create output filename\n",
        "    base_name = os.path.splitext(h5_file_path)[0]\n",
        "    output_file = f\"{base_name}_embeddings.json\"\n",
        "\n",
        "    print(f\"🔍 Processing: {h5_file_path}\")\n",
        "    print(f\"💾 Output: {output_file}\")\n",
        "\n",
        "    try:\n",
        "        # Open H5 file and extract everything\n",
        "        with h5py.File(h5_file_path, 'r') as f:\n",
        "\n",
        "            print(f\"📊 Analyzing H5 file structure...\")\n",
        "\n",
        "            # Extract file attributes (model info, config, etc.)\n",
        "            file_attrs = {}\n",
        "            for attr_name in f.attrs.keys():\n",
        "                file_attrs[attr_name] = convert_to_json_safe(f.attrs[attr_name])\n",
        "\n",
        "            # Get all image IDs (top-level keys)\n",
        "            image_ids = list(f.keys())\n",
        "            print(f\"📸 Found {len(image_ids)} images in the file\")\n",
        "\n",
        "            # Extract embeddings for all images\n",
        "            all_embeddings = {}\n",
        "\n",
        "            for i, image_id in enumerate(image_ids):\n",
        "                if i % 10 == 0:  # Progress update every 10 images\n",
        "                    print(f\"   Processing image {i+1}/{len(image_ids)}: {image_id}\")\n",
        "\n",
        "                image_group = f[image_id]\n",
        "                image_embeddings = extract_embeddings_from_group(image_group)\n",
        "                all_embeddings[image_id] = image_embeddings\n",
        "\n",
        "            # Create final structure\n",
        "            result = {\n",
        "                'extraction_info': {\n",
        "                    'source_file': h5_file_path,\n",
        "                    'extraction_date': datetime.now().isoformat(),\n",
        "                    'total_images': len(image_ids),\n",
        "                    'embedding_type': 'qwen2_5_vl_targeted_embeddings'\n",
        "                },\n",
        "                'file_attributes': file_attrs,\n",
        "                'embeddings': all_embeddings\n",
        "            }\n",
        "\n",
        "            # Save to JSON\n",
        "            print(f\"💾 Saving embeddings to JSON...\")\n",
        "            with open(output_file, 'w') as json_file:\n",
        "                json.dump(result, json_file, indent=2)\n",
        "\n",
        "            print(f\"✅ Success! Extracted embeddings to {output_file}\")\n",
        "\n",
        "            # Show file sizes and summary\n",
        "            h5_size = os.path.getsize(h5_file_path) / 1024 / 1024\n",
        "            json_size = os.path.getsize(output_file) / 1024 / 1024\n",
        "\n",
        "            print(f\"\\n📊 Summary:\")\n",
        "            print(f\"   📁 H5 file size: {h5_size:.2f} MB\")\n",
        "            print(f\"   📁 JSON file size: {json_size:.2f} MB\")\n",
        "            print(f\"   📸 Total images: {len(image_ids)}\")\n",
        "            print(f\"   🧠 Embeddings per image:\")\n",
        "\n",
        "            # Show structure of first image as example\n",
        "            if image_ids:\n",
        "                first_image = image_ids[0]\n",
        "                first_embeddings = all_embeddings[first_image]\n",
        "\n",
        "                if 'vision_embeddings' in first_embeddings:\n",
        "                    vision_shape = len(first_embeddings['vision_embeddings'])\n",
        "                    print(f\"      • Vision embeddings: {vision_shape} dimensions\")\n",
        "\n",
        "                if 'pre_generation' in first_embeddings:\n",
        "                    layers = list(first_embeddings['pre_generation'].keys())\n",
        "                    print(f\"      • Pre-generation layers: {len(layers)} ({layers})\")\n",
        "\n",
        "                    if layers:\n",
        "                        first_layer = first_embeddings['pre_generation'][layers[0]]\n",
        "                        if 'after_image_embeddings' in first_layer:\n",
        "                            after_img_shape = len(first_layer['after_image_embeddings'])\n",
        "                            print(f\"        - After image embeddings: {after_img_shape} dimensions\")\n",
        "                        if 'end_query_embeddings' in first_layer:\n",
        "                            end_query_shape = len(first_layer['end_query_embeddings'])\n",
        "                            print(f\"        - End query embeddings: {end_query_shape} dimensions\")\n",
        "\n",
        "                if 'token_positions' in first_embeddings:\n",
        "                    positions = first_embeddings['token_positions']\n",
        "                    print(f\"      • Token positions:\")\n",
        "                    for key, value in positions.items():\n",
        "                        print(f\"        - {key}: {value}\")\n",
        "\n",
        "            return output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing H5 file: {e}\")\n",
        "        return None\n",
        "\n",
        "def inspect_embeddings_structure(json_file_path):\n",
        "    \"\"\"Quick inspection of extracted embeddings JSON file.\"\"\"\n",
        "\n",
        "    if not os.path.exists(json_file_path):\n",
        "        print(f\"❌ JSON file not found: {json_file_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n🔍 Inspecting embeddings structure: {json_file_path}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        with open(json_file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Show extraction info\n",
        "        if 'extraction_info' in data:\n",
        "            info = data['extraction_info']\n",
        "            print(f\"📋 Extraction Info:\")\n",
        "            for key, value in info.items():\n",
        "                print(f\"   {key}: {value}\")\n",
        "\n",
        "        # Show file attributes\n",
        "        if 'file_attributes' in data:\n",
        "            attrs = data['file_attributes']\n",
        "            print(f\"\\n📋 File Attributes:\")\n",
        "            for key, value in attrs.items():\n",
        "                if isinstance(value, str) and len(value) > 100:\n",
        "                    print(f\"   {key}: {value[:100]}...\")\n",
        "                else:\n",
        "                    print(f\"   {key}: {value}\")\n",
        "\n",
        "        # Show embeddings structure\n",
        "        if 'embeddings' in data:\n",
        "            embeddings = data['embeddings']\n",
        "            print(f\"\\n🧠 Embeddings Structure:\")\n",
        "            print(f\"   Total images: {len(embeddings)}\")\n",
        "\n",
        "            # Show first image structure\n",
        "            if embeddings:\n",
        "                first_image_id = list(embeddings.keys())[0]\n",
        "                first_image_data = embeddings[first_image_id]\n",
        "\n",
        "                print(f\"   Sample image ({first_image_id}):\")\n",
        "                for key, value in first_image_data.items():\n",
        "                    if key == 'vision_embeddings' and isinstance(value, list):\n",
        "                        print(f\"     • {key}: {len(value)} dimensions\")\n",
        "                    elif key == 'pre_generation' and isinstance(value, dict):\n",
        "                        print(f\"     • {key}: {len(value)} layers\")\n",
        "                        for layer_name, layer_data in value.items():\n",
        "                            print(f\"       - {layer_name}: {list(layer_data.keys())}\")\n",
        "                    elif key == 'token_positions' and isinstance(value, dict):\n",
        "                        print(f\"     • {key}: {list(value.keys())}\")\n",
        "                    else:\n",
        "                        print(f\"     • {key}: {type(value).__name__}\")\n",
        "\n",
        "        file_size = os.path.getsize(json_file_path) / 1024 / 1024\n",
        "        print(f\"\\n📁 File size: {file_size:.2f} MB\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error inspecting JSON file: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with command line interface.\"\"\"\n",
        "\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Qwen2.5-VL Embeddings Extractor\")\n",
        "        print(\"=\" * 40)\n",
        "        print(\"\\nUsage:\")\n",
        "        print(\"  python h5_to_json_embeddings.py <h5_file_path>\")\n",
        "        print(\"\\nExample:\")\n",
        "        print(\"  python h5_to_json_embeddings.py qwen2_5_vl_targeted_embeddings_part1.h5\")\n",
        "        print(\"\\nThis will create:\")\n",
        "        print(\"  qwen2_5_vl_targeted_embeddings_part1_embeddings.json\")\n",
        "        return\n",
        "\n",
        "    h5_file_path = sys.argv[1]\n",
        "\n",
        "    # Extract embeddings\n",
        "    output_json = extract_qwen_embeddings_to_json(h5_file_path)\n",
        "\n",
        "    # Inspect the results\n",
        "    if output_json:\n",
        "        inspect_embeddings_structure(output_json)\n",
        "        print(f\"\\n🎉 Extraction completed successfully!\")\n",
        "        print(f\"📁 Output file: {output_json}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "oJRkqtIoHc7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import traceback\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from collections import defaultdict, Counter\n",
        "import requests\n",
        "import zipfile\n",
        "import re\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"Starting Qwen2.5-VL Caption Scoring Pipeline...\")\n",
        "\n",
        "# Download required NLTK data\n",
        "print(\"Downloading NLTK data...\")\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Paths configuration\n",
        "BASE_DIR = \"/root/akhil_workspace/qwen_model\"\n",
        "CAPTIONS_DIR = os.path.join(BASE_DIR, \"extracted_captions\")\n",
        "ANALYSIS_OUTPUT_DIR = os.path.join(BASE_DIR, \"factual_analysis\")\n",
        "COCO_DATA_DIR = os.path.join(BASE_DIR, \"coco_annotations\")\n",
        "EVALUATION_OUTPUT_DIR = os.path.join(BASE_DIR, \"evaluation_results\")\n",
        "\n",
        "# File paths\n",
        "QWEN_CAPTIONS_PATH = \"/root/akhil_workspace/qwen_model/extracted_captions/qwen2_captions.json\"\n",
        "COCO_ANNOTATIONS_PATH = os.path.join(COCO_DATA_DIR, \"captions_val2014.json\")\n",
        "OUTPUT_CSV_PATH = os.path.join(EVALUATION_OUTPUT_DIR, \"qwen2_comprehensive_scores.csv\")\n",
        "SUMMARY_PATH = os.path.join(EVALUATION_OUTPUT_DIR, \"qwen2_scoring_summary.json\")\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(ANALYSIS_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(EVALUATION_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(COCO_DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Initialize BERT model for factual accuracy\n",
        "print(\"Loading BERT model...\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "bert_model.eval()\n",
        "\n",
        "# CHAIR Metric Configuration - COCO 80 object categories\n",
        "MSCOCO_OBJECTS_80 = sorted([\n",
        "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
        "    'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "])\n",
        "MSCOCO_OBJECT_SET = set(MSCOCO_OBJECTS_80)\n",
        "\n",
        "def download_coco_annotations():\n",
        "    \"\"\"Download COCO 2014 validation annotations if not present.\"\"\"\n",
        "    if os.path.exists(COCO_ANNOTATIONS_PATH):\n",
        "        print(\"✅ COCO annotations already exist\")\n",
        "        return True\n",
        "\n",
        "    print(\"📥 Downloading COCO 2014 validation annotations...\")\n",
        "\n",
        "    # COCO 2014 annotations URL\n",
        "    annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n",
        "    zip_path = os.path.join(COCO_DATA_DIR, \"annotations_trainval2014.zip\")\n",
        "\n",
        "    try:\n",
        "        # Download annotations\n",
        "        print(\"   Downloading annotations zip file...\")\n",
        "        response = requests.get(annotations_url, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        with open(zip_path, 'wb') as f, tqdm(\n",
        "            desc=\"Downloading\",\n",
        "            total=total_size,\n",
        "            unit='B',\n",
        "            unit_scale=True\n",
        "        ) as pbar:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "\n",
        "        # Extract annotations\n",
        "        print(\"   Extracting annotations...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(COCO_DATA_DIR)\n",
        "\n",
        "        # Move the captions file to expected location\n",
        "        extracted_captions = os.path.join(COCO_DATA_DIR, \"annotations\", \"captions_val2014.json\")\n",
        "        if os.path.exists(extracted_captions):\n",
        "            os.rename(extracted_captions, COCO_ANNOTATIONS_PATH)\n",
        "            print(\"✅ COCO annotations downloaded and extracted successfully\")\n",
        "\n",
        "        # Clean up\n",
        "        os.remove(zip_path)\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading COCO annotations: {e}\")\n",
        "        return False\n",
        "\n",
        "def normalize_image_id(image_id):\n",
        "    \"\"\"Normalize image ID by removing the prefix and keeping only the numeric part.\"\"\"\n",
        "    if '_' in str(image_id):\n",
        "        return str(image_id).split('_')[-1]\n",
        "    return str(image_id)\n",
        "\n",
        "def load_coco_ground_truth():\n",
        "    \"\"\"Load COCO ground truth captions.\"\"\"\n",
        "    print(\"📚 Loading COCO ground truth captions...\")\n",
        "\n",
        "    if not os.path.exists(COCO_ANNOTATIONS_PATH):\n",
        "        print(\"COCO annotations not found. Attempting to download...\")\n",
        "        if not download_coco_annotations():\n",
        "            raise FileNotFoundError(f\"Could not download COCO annotations\")\n",
        "\n",
        "    try:\n",
        "        with open(COCO_ANNOTATIONS_PATH, 'r') as f:\n",
        "            coco_data = json.load(f)\n",
        "\n",
        "        # Process COCO annotations\n",
        "        image_captions = defaultdict(list)\n",
        "\n",
        "        for annotation in coco_data['annotations']:\n",
        "            image_id = str(annotation['image_id']).zfill(12)\n",
        "            caption = annotation['caption'].strip()\n",
        "\n",
        "            # Store with normalized ID\n",
        "            normalized_id = normalize_image_id(image_id)\n",
        "            image_captions[normalized_id].append(caption)\n",
        "\n",
        "        print(f\"   Loaded {len(image_captions)} images with ground truth captions\")\n",
        "        print(f\"   Average captions per image: {np.mean([len(caps) for caps in image_captions.values()]):.1f}\")\n",
        "        print(f\"   Sample normalized IDs: {list(image_captions.keys())[:5]}\")\n",
        "\n",
        "        return dict(image_captions)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading COCO ground truth: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_qwen_captions():\n",
        "    \"\"\"Load Qwen2.5-VL generated captions.\"\"\"\n",
        "    print(\"🤖 Loading Qwen2.5-VL generated captions...\")\n",
        "\n",
        "    if not os.path.exists(QWEN_CAPTIONS_PATH):\n",
        "        raise FileNotFoundError(f\"Qwen captions not found: {QWEN_CAPTIONS_PATH}\")\n",
        "\n",
        "    try:\n",
        "        with open(QWEN_CAPTIONS_PATH, 'r') as f:\n",
        "            qwen_data = json.load(f)\n",
        "\n",
        "        # Extract captions from our format\n",
        "        if 'captions' in qwen_data:\n",
        "            captions_dict = qwen_data['captions']\n",
        "        else:\n",
        "            captions_dict = qwen_data\n",
        "\n",
        "        # Convert to simple format with normalized IDs\n",
        "        qwen_captions = {}\n",
        "        for image_id, caption_data in captions_dict.items():\n",
        "            # Normalize the image ID\n",
        "            normalized_id = normalize_image_id(image_id)\n",
        "\n",
        "            if isinstance(caption_data, dict) and 'generated_caption' in caption_data:\n",
        "                qwen_captions[normalized_id] = caption_data['generated_caption']\n",
        "            elif isinstance(caption_data, str):\n",
        "                qwen_captions[normalized_id] = caption_data\n",
        "            else:\n",
        "                print(f\"Warning: Unexpected format for image {image_id}\")\n",
        "\n",
        "        print(f\"   Loaded {len(qwen_captions)} Qwen2.5-VL generated captions\")\n",
        "        print(f\"   Sample normalized IDs: {list(qwen_captions.keys())[:5]}\")\n",
        "\n",
        "        return qwen_captions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading Qwen captions: {e}\")\n",
        "        raise\n",
        "\n",
        "def cosine_similarity_torch(tensor1, tensor2):\n",
        "    \"\"\"Compute cosine similarity using PyTorch.\"\"\"\n",
        "    tensor1_norm = F.normalize(tensor1.unsqueeze(0), p=2, dim=1)\n",
        "    tensor2_norm = F.normalize(tensor2.unsqueeze(0), p=2, dim=1)\n",
        "    similarity = torch.mm(tensor1_norm, tensor2_norm.t())\n",
        "    return similarity.item()\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    \"\"\"Get BERT embedding for a text.\"\"\"\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            text = str(text).strip()\n",
        "            if not text:\n",
        "                return None\n",
        "\n",
        "            inputs = bert_tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(device)\n",
        "\n",
        "            outputs = bert_model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "            return embeddings.squeeze(0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting BERT embedding for text '{text[:50]}...': {e}\")\n",
        "        return None\n",
        "\n",
        "def compute_factual_score(generated_caption, ground_truth_captions):\n",
        "    \"\"\"Compute factual score using semantic similarity with ground truth captions.\"\"\"\n",
        "    try:\n",
        "        if not generated_caption or not ground_truth_captions:\n",
        "            return 0.0\n",
        "\n",
        "        gen_embedding = get_bert_embedding(generated_caption)\n",
        "        if gen_embedding is None:\n",
        "            return 0.0\n",
        "\n",
        "        similarities = []\n",
        "        for gt_caption in ground_truth_captions:\n",
        "            gt_embedding = get_bert_embedding(gt_caption)\n",
        "            if gt_embedding is not None:\n",
        "                similarity = cosine_similarity_torch(gen_embedding, gt_embedding)\n",
        "                similarities.append(float(similarity))\n",
        "\n",
        "        return float(max(similarities)) if similarities else 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing factual score: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def extract_mscoco_objects_from_caption(caption_text):\n",
        "    \"\"\"Extract MSCOCO objects mentioned in a caption using the COCO object list.\"\"\"\n",
        "    words = re.findall(r'\\b\\w+\\b', caption_text.lower())\n",
        "    mentioned_objects = set()\n",
        "\n",
        "    for word in words:\n",
        "        if word in MSCOCO_OBJECT_SET:\n",
        "            mentioned_objects.add(word)\n",
        "\n",
        "    return mentioned_objects\n",
        "\n",
        "def calculate_chair_metrics_individual(generated_caption, ground_truth_captions):\n",
        "    \"\"\"Calculate CHAIR metrics for a single image.\"\"\"\n",
        "    # Extract ground truth objects\n",
        "    gt_objects = set()\n",
        "    for caption in ground_truth_captions:\n",
        "        gt_objects.update(extract_mscoco_objects_from_caption(caption))\n",
        "\n",
        "    # Extract mentioned objects from generated caption\n",
        "    mentioned_objects = extract_mscoco_objects_from_caption(generated_caption)\n",
        "\n",
        "    # Find hallucinated objects (mentioned but not in ground truth)\n",
        "    hallucinated_objects = mentioned_objects - gt_objects\n",
        "\n",
        "    return {\n",
        "        'mentioned_objects': list(mentioned_objects),\n",
        "        'hallucinated_objects': list(hallucinated_objects),\n",
        "        'ground_truth_objects': list(gt_objects),\n",
        "        'has_hallucination': len(hallucinated_objects) > 0\n",
        "    }\n",
        "\n",
        "def calculate_standard_metrics_individual(generated_caption, ground_truth_captions):\n",
        "    \"\"\"Calculate standard captioning metrics for a single image.\"\"\"\n",
        "    smoothing = SmoothingFunction()\n",
        "\n",
        "    # Tokenize\n",
        "    gen_tokens = generated_caption.lower().split()\n",
        "    gt_tokens_list = [gt.lower().split() for gt in ground_truth_captions]\n",
        "\n",
        "    # BLEU scores\n",
        "    try:\n",
        "        bleu_1 = sentence_bleu(gt_tokens_list, gen_tokens,\n",
        "                             weights=(1.0, 0, 0, 0),\n",
        "                             smoothing_function=smoothing.method1)\n",
        "    except:\n",
        "        bleu_1 = 0.0\n",
        "\n",
        "    try:\n",
        "        bleu_4 = sentence_bleu(gt_tokens_list, gen_tokens,\n",
        "                             weights=(0.25, 0.25, 0.25, 0.25),\n",
        "                             smoothing_function=smoothing.method1)\n",
        "    except:\n",
        "        bleu_4 = 0.0\n",
        "\n",
        "    # Simple ROUGE-L implementation\n",
        "    def simple_rouge_l(candidate, reference):\n",
        "        def lcs_length(x, y):\n",
        "            m, n = len(x), len(y)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if x[i-1] == y[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                    else:\n",
        "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "\n",
        "            return dp[m][n]\n",
        "\n",
        "        candidate_words = candidate.split()\n",
        "        reference_words = reference.split()\n",
        "\n",
        "        if not candidate_words or not reference_words:\n",
        "            return 0.0\n",
        "\n",
        "        lcs_len = lcs_length(candidate_words, reference_words)\n",
        "\n",
        "        if lcs_len == 0:\n",
        "            return 0.0\n",
        "\n",
        "        precision = lcs_len / len(candidate_words)\n",
        "        recall = lcs_len / len(reference_words)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "    # ROUGE-L score\n",
        "    rouge_vals = []\n",
        "    for gt_caption in ground_truth_captions:\n",
        "        rouge = simple_rouge_l(generated_caption.lower(), gt_caption.lower())\n",
        "        rouge_vals.append(rouge)\n",
        "    rouge_l = max(rouge_vals) if rouge_vals else 0.0\n",
        "\n",
        "    return {\n",
        "        'BLEU-1': bleu_1,\n",
        "        'BLEU-4': bleu_4,\n",
        "        'ROUGE-L': rouge_l\n",
        "    }\n",
        "\n",
        "def analyze_captions():\n",
        "    \"\"\"Main analysis function.\"\"\"\n",
        "    print(\"🔬 Starting comprehensive caption analysis...\")\n",
        "\n",
        "    # Load data\n",
        "    ground_truth_captions = load_coco_ground_truth()\n",
        "    qwen_captions = load_qwen_captions()\n",
        "\n",
        "    # Find common images using normalized IDs\n",
        "    common_images = set(ground_truth_captions.keys()) & set(qwen_captions.keys())\n",
        "    print(f\"📊 Found {len(common_images)} images with both Qwen and ground truth captions\")\n",
        "\n",
        "    if len(common_images) == 0:\n",
        "        print(\"❌ No common images found even after normalization! Check image ID formats.\")\n",
        "        print(f\"Sample Qwen normalized IDs: {list(qwen_captions.keys())[:5]}\")\n",
        "        print(f\"Sample GT normalized IDs: {list(ground_truth_captions.keys())[:5]}\")\n",
        "        return\n",
        "\n",
        "    # Analyze each image\n",
        "    results = []\n",
        "\n",
        "    print(\"🧮 Computing comprehensive scores...\")\n",
        "    for image_id in tqdm(sorted(common_images), desc=\"Analyzing images\"):\n",
        "        try:\n",
        "            generated_caption = qwen_captions[image_id]\n",
        "            gt_captions = ground_truth_captions[image_id]\n",
        "\n",
        "            # Compute factual score\n",
        "            factual_score = compute_factual_score(generated_caption, gt_captions)\n",
        "\n",
        "            # Compute CHAIR metrics\n",
        "            chair_metrics = calculate_chair_metrics_individual(generated_caption, gt_captions)\n",
        "\n",
        "            # Compute standard metrics\n",
        "            standard_metrics = calculate_standard_metrics_individual(generated_caption, gt_captions)\n",
        "\n",
        "            # Create comprehensive result\n",
        "            result = {\n",
        "                'image_id': image_id,\n",
        "                'generated_caption': generated_caption,\n",
        "                'ground_truth_captions': '; '.join(gt_captions),\n",
        "                'factual_score': factual_score,\n",
        "                'has_hallucination': chair_metrics['has_hallucination'],\n",
        "                'mentioned_objects': '; '.join(chair_metrics['mentioned_objects']),\n",
        "                'hallucinated_objects': '; '.join(chair_metrics['hallucinated_objects']),\n",
        "                'ground_truth_objects': '; '.join(chair_metrics['ground_truth_objects']),\n",
        "                'BLEU-1': standard_metrics['BLEU-1'],\n",
        "                'BLEU-4': standard_metrics['BLEU-4'],\n",
        "                'ROUGE-L': standard_metrics['ROUGE-L']\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing image {image_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Add the requested additional columns\n",
        "    print(\"📊 Adding additional computed columns...\")\n",
        "\n",
        "    df[\"hallucinated_objects_count\"] = df[\"hallucinated_objects\"].apply(\n",
        "        lambda x: len(x.split(\";\")) if pd.notna(x) and x.strip() else 0\n",
        "    )\n",
        "    df[\"mentioned_objects_count\"] = df[\"mentioned_objects\"].apply(\n",
        "        lambda x: len(x.split(\";\")) if pd.notna(x) and x.strip() else 0\n",
        "    )\n",
        "    df[\"ground_truth_objects_count\"] = df[\"ground_truth_objects\"].apply(\n",
        "        lambda x: len(x.split(\";\")) if pd.notna(x) and x.strip() else 0\n",
        "    )\n",
        "\n",
        "    df[\"chair_regression_score\"] = np.where(\n",
        "        df[\"mentioned_objects_count\"] == 0,\n",
        "        0,\n",
        "        df[\"hallucinated_objects_count\"] / df[\"mentioned_objects_count\"]\n",
        "    )\n",
        "\n",
        "    # Compute overall statistics\n",
        "    stats = {\n",
        "        'total_images': len(df),\n",
        "        'average_factual_score': float(df['factual_score'].mean()),\n",
        "        'hallucination_rate': float(df['has_hallucination'].mean()),\n",
        "        'average_chair_regression_score': float(df['chair_regression_score'].mean()),\n",
        "        'average_bleu_1': float(df['BLEU-1'].mean()),\n",
        "        'average_bleu_4': float(df['BLEU-4'].mean()),\n",
        "        'average_rouge_l': float(df['ROUGE-L'].mean()),\n",
        "        'average_mentioned_objects': float(df['mentioned_objects_count'].mean()),\n",
        "        'average_hallucinated_objects': float(df['hallucinated_objects_count'].mean()),\n",
        "        'average_ground_truth_objects': float(df['ground_truth_objects_count'].mean())\n",
        "    }\n",
        "\n",
        "    # Save CSV file\n",
        "    print(f\"💾 Saving comprehensive scores to CSV...\")\n",
        "    df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
        "\n",
        "    # Save summary statistics\n",
        "    summary_data = {\n",
        "        'analysis_date': pd.Timestamp.now().isoformat(),\n",
        "        'model_type': 'Qwen2.5-VL',\n",
        "        'total_images_analyzed': len(df),\n",
        "        'statistics': stats,\n",
        "        'column_descriptions': {\n",
        "            'factual_score': 'BERT-based semantic similarity with ground truth (0-1)',\n",
        "            'has_hallucination': 'Boolean indicating presence of hallucinated objects',\n",
        "            'chair_regression_score': 'Ratio of hallucinated to mentioned objects (0-1)',\n",
        "            'mentioned_objects_count': 'Number of COCO objects mentioned in caption',\n",
        "            'hallucinated_objects_count': 'Number of COCO objects mentioned but not in ground truth',\n",
        "            'ground_truth_objects_count': 'Number of COCO objects in ground truth captions',\n",
        "            'BLEU-1': 'BLEU-1 score (0-1)',\n",
        "            'BLEU-4': 'BLEU-4 score (0-1)',\n",
        "            'ROUGE-L': 'ROUGE-L F1 score (0-1)'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(SUMMARY_PATH, 'w') as f:\n",
        "        json.dump(summary_data, f, indent=2)\n",
        "\n",
        "    return df, stats\n",
        "\n",
        "def print_analysis_summary(df, stats):\n",
        "    \"\"\"Print a comprehensive summary of the analysis.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"📊 QWEN2.5-VL COMPREHENSIVE ANALYSIS SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(f\"🔢 Total Images Analyzed: {stats['total_images']}\")\n",
        "    print(f\"📈 Average Factual Score: {stats['average_factual_score']:.4f}\")\n",
        "    print(f\"🚨 Hallucination Rate: {stats['hallucination_rate']:.4f} ({stats['hallucination_rate']*100:.1f}%)\")\n",
        "    print(f\"📊 Average CHAIR Regression Score: {stats['average_chair_regression_score']:.4f}\")\n",
        "\n",
        "    print(f\"\\n📝 Standard Captioning Metrics:\")\n",
        "    print(f\"   BLEU-1: {stats['average_bleu_1']:.4f}\")\n",
        "    print(f\"   BLEU-4: {stats['average_bleu_4']:.4f}\")\n",
        "    print(f\"   ROUGE-L: {stats['average_rouge_l']:.4f}\")\n",
        "\n",
        "    print(f\"\\n🔍 Object Detection Analysis:\")\n",
        "    print(f\"   Average Mentioned Objects: {stats['average_mentioned_objects']:.2f}\")\n",
        "    print(f\"   Average Hallucinated Objects: {stats['average_hallucinated_objects']:.2f}\")\n",
        "    print(f\"   Average Ground Truth Objects: {stats['average_ground_truth_objects']:.2f}\")\n",
        "\n",
        "    # Show best and worst examples\n",
        "    print(f\"\\n🏆 Best Examples (Highest Factual Score):\")\n",
        "    best_examples = df.nlargest(3, 'factual_score')\n",
        "    for i, (_, row) in enumerate(best_examples.iterrows()):\n",
        "        print(f\"   {i+1}. Factual Score: {row['factual_score']:.4f}, CHAIR: {row['chair_regression_score']:.4f}\")\n",
        "        print(f\"      Generated: '{row['generated_caption'][:100]}...'\")\n",
        "        print(f\"      Hallucinated Objects: {row['hallucinated_objects_count']}\")\n",
        "\n",
        "    print(f\"\\n🔍 Worst Examples (Lowest Factual Score):\")\n",
        "    worst_examples = df.nsmallest(3, 'factual_score')\n",
        "    for i, (_, row) in enumerate(worst_examples.iterrows()):\n",
        "        print(f\"   {i+1}. Factual Score: {row['factual_score']:.4f}, CHAIR: {row['chair_regression_score']:.4f}\")\n",
        "        print(f\"      Generated: '{row['generated_caption'][:100]}...'\")\n",
        "        print(f\"      Hallucinated Objects: {row['hallucinated_objects_count']}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function.\"\"\"\n",
        "    try:\n",
        "        print(\"🚀 Starting Qwen2.5-VL Comprehensive Scoring Pipeline\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Check if Qwen captions exist\n",
        "        if not os.path.exists(QWEN_CAPTIONS_PATH):\n",
        "            print(f\"❌ Qwen captions file not found: {QWEN_CAPTIONS_PATH}\")\n",
        "            print(\"   Please run the caption extraction script first!\")\n",
        "            return\n",
        "\n",
        "        # Run analysis\n",
        "        df, stats = analyze_captions()\n",
        "\n",
        "        if df is not None and len(df) > 0:\n",
        "            # Print summary\n",
        "            print_analysis_summary(df, stats)\n",
        "\n",
        "            print(f\"\\n📁 Output Files:\")\n",
        "            print(f\"   Comprehensive CSV: {OUTPUT_CSV_PATH}\")\n",
        "            print(f\"   Summary Statistics: {SUMMARY_PATH}\")\n",
        "\n",
        "            print(f\"\\n📊 CSV File Contains {len(df)} rows with columns:\")\n",
        "            for col in df.columns:\n",
        "                print(f\"   • {col}\")\n",
        "\n",
        "            print(f\"\\n✨ Analysis completed successfully!\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"\\n⚠️  Analysis interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error in main: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "eObBuF1IHc5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "def normalize_image_id(image_id):\n",
        "    \"\"\"Normalize image ID for matching.\"\"\"\n",
        "    if isinstance(image_id, str) and '_' in image_id:\n",
        "        return image_id.split('_')[-1]  # Extract numeric part from H5\n",
        "\n",
        "    if isinstance(image_id, (int, float)):\n",
        "        return str(int(image_id)).zfill(12)  # Zero-pad CSV IDs\n",
        "\n",
        "    return str(image_id)\n",
        "\n",
        "def load_scores_data(csv_path):\n",
        "    \"\"\"Load and prepare scores data.\"\"\"\n",
        "    print(\"📊 Loading scores data...\")\n",
        "\n",
        "    scores_df = pd.read_csv(csv_path)\n",
        "    print(f\"   Loaded {len(scores_df)} rows\")\n",
        "\n",
        "    # Normalize IDs for matching\n",
        "    scores_df['norm_id'] = scores_df['image_id'].apply(normalize_image_id)\n",
        "\n",
        "    # Create lookup dictionary for fast access\n",
        "    scores_dict = {}\n",
        "    for _, row in scores_df.iterrows():\n",
        "        scores_dict[row['norm_id']] = {\n",
        "            'generated_caption': row.get('generated_caption', ''),\n",
        "            'ground_truth_captions': row.get('ground_truth_captions', ''),\n",
        "            'factual_score': row.get('factual_score', 0.0),\n",
        "            'has_hallucination': row.get('has_hallucination', False),\n",
        "            'BLEU-1': row.get('BLEU-1', 0.0),\n",
        "            'BLEU-4': row.get('BLEU-4', 0.0),\n",
        "            'ROUGE-L': row.get('ROUGE-L', 0.0),\n",
        "            'chair_regression_score': row.get('chair_regression_score', 0.0)\n",
        "        }\n",
        "\n",
        "    print(f\"   Created lookup dict for {len(scores_dict)} images\")\n",
        "    return scores_dict\n",
        "\n",
        "def discover_embedding_structure(embeddings_dir):\n",
        "    \"\"\"Discover available embedding types.\"\"\"\n",
        "    print(\"🔍 Discovering embedding structure...\")\n",
        "\n",
        "    h5_files = [f for f in os.listdir(embeddings_dir) if f.endswith('.h5')]\n",
        "    if not h5_files:\n",
        "        return {}\n",
        "\n",
        "    # Check first file\n",
        "    sample_file = os.path.join(embeddings_dir, h5_files[0])\n",
        "    configs = {}\n",
        "\n",
        "    with h5py.File(sample_file, 'r') as f:\n",
        "        sample_image = list(f.keys())[0]\n",
        "        img_group = f[sample_image]\n",
        "\n",
        "        print(f\"   Sample image: {sample_image}\")\n",
        "        print(f\"   Available keys: {list(img_group.keys())}\")\n",
        "\n",
        "        # Vision embeddings\n",
        "        if 'vision_embeddings' in img_group:\n",
        "            configs['vision_embeddings'] = ['vision_embeddings']\n",
        "            print(f\"   ✅ Found vision_embeddings: {img_group['vision_embeddings'].shape}\")\n",
        "\n",
        "        # Pre-generation embeddings\n",
        "        if 'pre_generation' in img_group:\n",
        "            pre_gen = img_group['pre_generation']\n",
        "            layers = list(pre_gen.keys())\n",
        "            print(f\"   📥 Pre-generation layers: {layers}\")\n",
        "\n",
        "            for layer in layers:\n",
        "                layer_group = pre_gen[layer]\n",
        "                emb_types = list(layer_group.keys())\n",
        "                print(f\"     {layer}: {emb_types}\")\n",
        "\n",
        "                for emb_type in emb_types:\n",
        "                    config_name = f\"pre_generation_{layer}_{emb_type.replace('_embeddings', '')}\"\n",
        "                    configs[config_name] = ['pre_generation', layer, emb_type]\n",
        "\n",
        "    print(f\"   📋 Discovered {len(configs)} embedding types\")\n",
        "    return configs\n",
        "\n",
        "def extract_embedding(img_group, path):\n",
        "    \"\"\"Extract embedding from H5 group using path.\"\"\"\n",
        "    try:\n",
        "        current = img_group\n",
        "        for step in path:\n",
        "            if step in current:\n",
        "                current = current[step]\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        # Convert to numpy array\n",
        "        if hasattr(current, 'shape'):\n",
        "            return np.array(current)\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def process_single_config(config_name, path, embeddings_dir, scores_dict, output_dir, chunk_size=1000):\n",
        "    \"\"\"Process a single embedding configuration with chunked processing.\"\"\"\n",
        "    print(f\"\\n🔨 Processing: {config_name}\")\n",
        "    print(f\"   Path: {' -> '.join(path)}\")\n",
        "\n",
        "    # Get all H5 files\n",
        "    h5_files = [f for f in os.listdir(embeddings_dir) if f.endswith('.h5')]\n",
        "    h5_files.sort()  # Process in order\n",
        "\n",
        "    output_file = os.path.join(output_dir, f\"qwen_dataset_{config_name}.csv\")\n",
        "\n",
        "    # Process files in chunks to avoid memory issues\n",
        "    all_rows = []\n",
        "    processed_count = 0\n",
        "    matched_count = 0\n",
        "\n",
        "    for h5_file in tqdm(h5_files, desc=f\"Processing {config_name}\"):\n",
        "        file_path = os.path.join(embeddings_dir, h5_file)\n",
        "\n",
        "        try:\n",
        "            with h5py.File(file_path, 'r') as f:\n",
        "                image_ids = list(f.keys())\n",
        "\n",
        "                for image_id in image_ids:\n",
        "                    try:\n",
        "                        img_group = f[image_id]\n",
        "\n",
        "                        # Extract embedding\n",
        "                        embedding = extract_embedding(img_group, path)\n",
        "\n",
        "                        if embedding is not None:\n",
        "                            processed_count += 1\n",
        "\n",
        "                            # Normalize ID and check for scores\n",
        "                            norm_id = normalize_image_id(image_id)\n",
        "\n",
        "                            if norm_id in scores_dict:\n",
        "                                matched_count += 1\n",
        "                                scores = scores_dict[norm_id]\n",
        "\n",
        "                                row = {\n",
        "                                    'image_id': image_id,\n",
        "                                    'corresponding_embedding': embedding.tolist(),\n",
        "                                    'generated_caption': scores['generated_caption'],\n",
        "                                    'ground_truth_captions': scores['ground_truth_captions'],\n",
        "                                    'factual_score': scores['factual_score'],\n",
        "                                    'has_hallucination': scores['has_hallucination'],\n",
        "                                    'BLEU-1': scores['BLEU-1'],\n",
        "                                    'BLEU-4': scores['BLEU-4'],\n",
        "                                    'ROUGE-L': scores['ROUGE-L'],\n",
        "                                    'chair_regression_score': scores['chair_regression_score']\n",
        "                                }\n",
        "\n",
        "                                all_rows.append(row)\n",
        "\n",
        "                                # Save chunk to avoid memory buildup\n",
        "                                if len(all_rows) >= chunk_size:\n",
        "                                    save_chunk(all_rows, output_file, is_first=(matched_count <= chunk_size))\n",
        "                                    all_rows = []\n",
        "                                    gc.collect()  # Force garbage collection\n",
        "\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error processing {h5_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Save remaining rows\n",
        "    if all_rows:\n",
        "        save_chunk(all_rows, output_file, is_first=(matched_count <= len(all_rows)))\n",
        "\n",
        "    print(f\"   ✅ Processed {processed_count} embeddings, matched {matched_count}\")\n",
        "\n",
        "    if matched_count > 0:\n",
        "        print(f\"   💾 Saved to: {output_file}\")\n",
        "\n",
        "        # Create summary\n",
        "        create_summary(output_file, config_name, path, matched_count)\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"   ❌ No matches found for {config_name}\")\n",
        "        return False\n",
        "\n",
        "def save_chunk(rows, output_file, is_first=True):\n",
        "    \"\"\"Save a chunk of rows to CSV file.\"\"\"\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Write header only for first chunk\n",
        "    mode = 'w' if is_first else 'a'\n",
        "    header = is_first\n",
        "\n",
        "    df.to_csv(output_file, mode=mode, header=header, index=False)\n",
        "\n",
        "def create_summary(csv_file, config_name, path, total_samples):\n",
        "    \"\"\"Create summary statistics for the dataset.\"\"\"\n",
        "    try:\n",
        "        # Read just a sample to get embedding dimension\n",
        "        sample_df = pd.read_csv(csv_file, nrows=1)\n",
        "        embedding_dim = len(eval(sample_df['corresponding_embedding'].iloc[0]))\n",
        "\n",
        "        # Read full file for statistics (only if not too large)\n",
        "        if total_samples < 10000:\n",
        "            full_df = pd.read_csv(csv_file)\n",
        "\n",
        "            summary = {\n",
        "                'dataset_name': f\"qwen_dataset_{config_name}\",\n",
        "                'total_samples': total_samples,\n",
        "                'embedding_dimension': embedding_dim,\n",
        "                'embedding_path': path,\n",
        "                'statistics': {\n",
        "                    'factual_score': {\n",
        "                        'mean': float(full_df['factual_score'].mean()),\n",
        "                        'std': float(full_df['factual_score'].std()),\n",
        "                        'min': float(full_df['factual_score'].min()),\n",
        "                        'max': float(full_df['factual_score'].max())\n",
        "                    },\n",
        "                    'hallucination_rate': float(full_df['has_hallucination'].mean()),\n",
        "                    'avg_bleu4': float(full_df['BLEU-4'].mean()),\n",
        "                    'avg_rouge_l': float(full_df['ROUGE-L'].mean())\n",
        "                }\n",
        "            }\n",
        "        else:\n",
        "            # For large files, create basic summary\n",
        "            summary = {\n",
        "                'dataset_name': f\"qwen_dataset_{config_name}\",\n",
        "                'total_samples': total_samples,\n",
        "                'embedding_dimension': embedding_dim,\n",
        "                'embedding_path': path,\n",
        "                'note': 'Statistics not computed for large dataset to save memory'\n",
        "            }\n",
        "\n",
        "        # Save summary\n",
        "        summary_file = csv_file.replace('.csv', '_summary.json')\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Warning: Could not create summary: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function - simple and robust.\"\"\"\n",
        "\n",
        "    # Paths\n",
        "    embeddings_dir = \"/root/akhil_workspace/qwen_model/extracted_embeddings_coco2014\"\n",
        "    scores_csv_path = \"/root/akhil_workspace/qwen_model/evaluation_results/qwen2_comprehensive_scores.csv\"\n",
        "    output_dir = \"/root/akhil_workspace/qwen_model/comprehensive_datasets\"\n",
        "\n",
        "    print(\"🚀 Simple Qwen Dataset Creation\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Features:\")\n",
        "    print(\"  • Memory-efficient chunked processing\")\n",
        "    print(\"  • No complex GPU operations\")\n",
        "    print(\"  • Robust error handling\")\n",
        "    print(\"  • Saves datasets incrementally\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check paths\n",
        "    if not os.path.exists(embeddings_dir):\n",
        "        print(f\"❌ Embeddings directory not found: {embeddings_dir}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(scores_csv_path):\n",
        "        print(f\"❌ Scores CSV not found: {scores_csv_path}\")\n",
        "        return\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load scores data\n",
        "    scores_dict = load_scores_data(scores_csv_path)\n",
        "\n",
        "    # Discover embedding structure\n",
        "    configs = discover_embedding_structure(embeddings_dir)\n",
        "\n",
        "    if not configs:\n",
        "        print(\"❌ No embedding configurations found!\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n🎯 Will create {len(configs)} datasets:\")\n",
        "    for config_name in configs.keys():\n",
        "        print(f\"   • {config_name}\")\n",
        "\n",
        "    # Process each configuration\n",
        "    successful = 0\n",
        "    failed = 0\n",
        "\n",
        "    for config_name, path in configs.items():\n",
        "        try:\n",
        "            success = process_single_config(\n",
        "                config_name, path, embeddings_dir, scores_dict, output_dir\n",
        "            )\n",
        "\n",
        "            if success:\n",
        "                successful += 1\n",
        "            else:\n",
        "                failed += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Failed {config_name}: {e}\")\n",
        "            failed += 1\n",
        "\n",
        "        # Force cleanup after each config\n",
        "        gc.collect()\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\n🎉 Dataset Creation Complete!\")\n",
        "    print(f\"   ✅ Successful: {successful}\")\n",
        "    print(f\"   ❌ Failed: {failed}\")\n",
        "    print(f\"   📁 Output directory: {output_dir}\")\n",
        "\n",
        "    if successful > 0:\n",
        "        print(f\"\\n📋 Each dataset contains:\")\n",
        "        print(f\"   • image_id, corresponding_embedding, generated_caption\")\n",
        "        print(f\"   • ground_truth_captions, factual_score, has_hallucination\")\n",
        "        print(f\"   • BLEU-1, BLEU-4, ROUGE-L, chair_regression_score\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mSwzKqECHczS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZAl6dLpdHcwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 300 and 500_dataset_creation Missing"
      ],
      "metadata": {
        "id": "CWjsfEzZHctk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2rvwKbUPJPg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYBoL2nnJPeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vy_phmLlJPbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPN7_0sFJPY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wXlsQ6y3JPT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4vhMy3zJPQv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}