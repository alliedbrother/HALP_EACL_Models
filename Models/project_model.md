# HALP: hallucination prediction via probing (pre-generation) - VQA Edition

a research pipeline to predict vlm hallucinations in visual question answering before a single token is generated by probing internal states from one forward pass. the code extracts vision and decoder embeddings, processes VQA responses, computes hallucination/quality metrics using relational question analysis, and assembles learning-ready tables for training simple probes or running analyses on relational reasoning capabilities.

---

## project layout

```
.
├── Detecting_Vision_Language_Model_Hallucinations_before_Generation.pdf
├── 100_vqa_embedding_extraction.py      # cpu-friendly VQA embedding extractor (single forward pass per Q&A pair)
├── 100_vqa_extract_gpu.py               # gpu-optimized VQA extractor (same outputs; faster/safer memory handling)
├── 200_h5_to_json.py                    # optional: hdf5 → json mirror for debugging/inspection
├── 300_vqa_response_extracting.py       # consolidate generated VQA responses from .h5 files → vqa_responses.json
├── 400_vqa_scoring.py                   # compute relational accuracy, answer quality, hallucination metrics → scores.csv
├── 500_vqa_dataset_creation.py          # join embeddings + scores → per-embedding csv datasets (cpu)
└── 500_vqa_gpu_dataset_creation.py      # same as above but chunked & memory-safe (preferred for large data)
```

recommended output folders (created automatically as needed)

```
outputs/
  embeddings/          # .h5 VQA embedding shards
  embeddings_json/     # optional json mirrors
  vqa_responses/       # vqa_responses.json
  scores/              # comprehensive_vqa_scores.csv
  datasets/            # dataset_*.csv and summary_*.json
```

---

## quick start

1. create environment

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python - <<'PY'
import nltk
nltk.download('punkt')
PY
```

2. set data roots

```bash
export IMAGES_DIR=/Users/saiakhil/Documents/Thesis/HALP_EACL_Local/Data_Sets/final_data/all_images
export VQA_DATASET=/Users/saiakhil/Documents/Thesis/HALP_EACL_Local/Data_Sets/final_data/sampled_10k_relational_dataset.csv
mkdir -p outputs/{embeddings,embeddings_json,vqa_responses,scores,datasets}
```

3. run the pipeline

```bash
# 1) extract internal representations and store generated VQA responses
python 100_vqa_extract_gpu.py \
  --images $IMAGES_DIR \
  --vqa-dataset $VQA_DATASET \
  --out outputs/embeddings \
  --model qwen2_5_vl_7b \
  --layers auto \
  --device cuda \
  --chunk-size 256

# optional: create human-readable mirrors for quick inspection
python 200_h5_to_json.py \
  --h5-dir outputs/embeddings \
  --out-dir outputs/embeddings_json

# 2) harvest generated VQA responses from all h5 parts
python 300_vqa_response_extracting.py \
  --h5-dir outputs/embeddings \
  --out outputs/vqa_responses/vqa_responses.json

# 3) compute relational accuracy and hallucination metrics
python 400_vqa_scoring.py \
  --vqa-responses-json outputs/vqa_responses/vqa_responses.json \
  --vqa-dataset $VQA_DATASET \
  --out-csv outputs/scores/qwen2_vqa_comprehensive_scores.csv

# 4) assemble learning-ready tables (preferred gpu-safe variant)
python 500_vqa_gpu_dataset_creation.py \
  --h5-dir outputs/embeddings \
  --scores-csv outputs/scores/qwen2_vqa_comprehensive_scores.csv \
  --out-dir outputs/datasets
```

each script supports --help for full arguments.

---

## what each script does

### 100_vqa_embedding_extraction.py

purpose

* load a vlm (for example qwen2.5-vl, llava, blip, paligemma)
* run a single forward pass per VQA pair and persist internal representations plus the model's generated answer
* process questions from the relational VQA dataset with corresponding images

saves per question_id group in hdf5

* vision_embeddings: pooled final vision-encoder vector (d,)
* pre_generation/layer_<k>/after_image_embeddings: decoder hidden at position right after image tokens (d,)
* pre_generation/layer_<k>/after_question_embeddings: decoder hidden at position right after question tokens (d,)
* pre_generation/layer_<k>/end_query_embeddings: decoder hidden at final query token before decoding (d,)
* generated_answer: utf-8 string
* question_text: utf-8 string
* ground_truth_answer: utf-8 string
* is_relational: boolean flag
* token_positions: small json or indices
* file attributes include model_name, selected_layers, hidden_size, created_at

output

* outputs/embeddings/part_*.h5

notes

* batch size is effectively 1 for most vision towers; the gpu script handles chunking VQA pairs across iterations

### 100_vqa_extract_gpu.py

purpose

* same outputs and schema as 100_vqa_embedding_extraction.py but with safer memory usage and cuda acceleration
* optimized for processing large VQA datasets with relational questions

output

* identical hdf5 layout for drop-in equivalence

### 200_h5_to_json.py  (optional)

purpose

* recursively mirror each hdf5 file into a json tree for quick grepping and diffs

input → output

* embeddings/part_*.h5 → embeddings_json/part_*.json

### 300_vqa_response_extracting.py

purpose

* scan all hdf5 parts, collect each question_id's stored generated_answer, question_text, and ground_truth_answer, and build one consolidated json with minimal stats

output schema

```json
{
  "vqa_responses": {
    "question_comb_1": {
      "question_text": "How many sharks are present in the travel brochure?",
      "generated_answer": "There are no sharks visible in this travel brochure.",
      "ground_truth_answer": "There are no sharks in the brochure",
      "is_relational": true,
      "image_name": "haloquest_2082.png",
      "source_file": "part_03.h5"
    }
  },
  "metadata": { "files_processed": 17, "vqa_pairs": 10000 }
}
```

### 400_vqa_scoring.py

purpose

* compute per-question VQA accuracy and hallucination metrics with special focus on relational reasoning

metrics

* exact_match: binary exact match between generated and ground truth answers
* semantic_similarity: sentence-embedding cosine similarity between generated and ground truth
* relational_accuracy: accuracy specifically for questions marked as relational
* answer_length_ratio: ratio of generated answer length to ground truth
* hallucination_score: custom metric for detecting fabricated information in answers
* bleu-1..4, rouge-l for answer quality

input → output

* vqa_responses.json + sampled VQA dataset → scores/qwen2_vqa_comprehensive_scores.csv

example columns

```
question_id, question_text, generated_answer, ground_truth_answer, is_relational,
exact_match, semantic_similarity, relational_accuracy, hallucination_score,
bleu1, bleu4, rougeL, answer_length_ratio
```

### 500_vqa_dataset_creation.py

purpose

* join VQA embeddings from hdf5 with scores csv to produce one training table per embedding path
* loads all into ram; use for small to medium VQA datasets
* creates datasets optimized for relational reasoning probe training

output

* datasets/dataset_<embedding_path>.csv
* datasets/summary_<embedding_path>.json (count, mean scores, relational accuracy rate, etc.)

### 500_vqa_gpu_dataset_creation.py

purpose

* same as above but chunked and memory-safe for large-scale VQA data
* recommended for production with 10k+ VQA pairs

output

* identical filenames as cpu version

---

## data schemas

### hdf5 produced by 100_vqa_*

```
/<question_id>                      
  ├─ vision_embeddings                 float32 (d,)
  ├─ pre_generation/
  │   └─ layer_<k>/
  │       ├─ after_image_embeddings    float32 (d,)
  │       ├─ after_question_embeddings float32 (d,)
  │       └─ end_query_embeddings      float32 (d,)
  ├─ generated_answer                  utf-8 string
  ├─ question_text                     utf-8 string
  ├─ ground_truth_answer               utf-8 string
  ├─ is_relational                     boolean
  ├─ image_name                        utf-8 string
  └─ token_positions                   json or small array

file attrs: model_name, selected_layers, hidden_size, created_at, etc.
```

### scores csv produced by 400_vqa_scoring

* one row per question_id with at least

  * question_id
  * question_text
  * generated_answer
  * ground_truth_answer
  * is_relational
  * exact_match, semantic_similarity, relational_accuracy
  * hallucination_score, bleu1, bleu4, rougeL, answer_length_ratio

### final dataset rows produced by 500_vqa_*

* one row per question per embedding path

  * question_id
  * corresponding_embedding: json-encoded float array length d
  * question_text, generated_answer, ground_truth_answer
  * is_relational, exact_match, semantic_similarity, relational_accuracy
  * hallucination_score, bleu*, rougeL, answer_length_ratio

---

## requirements

if you do not already have a requirements.txt, start with this

```
python>=3.10
torch>=2.1
torchvision>=0.16
transformers>=4.41
accelerate>=0.29
timm>=0.9
pillow
numpy
pandas
h5py
scikit-learn
sentence-transformers
nltk
rouge-score
tqdm
```

some vlms may need extra wheels or bf16-capable gpus. install per model’s readme as needed.

---

## evaluation philosophy

* halp probes pre-generation signals in VQA tasks with focus on relational reasoning. we intentionally separate extraction, scoring, and dataset assembly so any stage can be rerun independently.
* layer and token-position ablations help test the hypothesis that hallucination signals differ across the stack and positions, particularly for relational vs non-relational questions.
* the 70/30 split of relational/non-relational questions allows for comparative analysis of model behavior on different question types.

---

## extending to relational reasoning analysis

goal

* quantify relational reasoning capabilities and hallucinations in VQA responses, leveraging the curated relational dataset

dataset advantages

* 10,000 VQA pairs with 70% relational questions across 6 diverse datasets (AMBER, HALOQUEST, POPE, MME, HALLUSIONBENCH, MATHVISTA)
* pre-labeled relational vs non-relational questions using comprehensive keyword analysis
* balanced representation ensuring robust evaluation across different visual domains

procedure outline

1. analyze generated VQA responses for relational reasoning patterns

   * spatial relationships: "between", "behind", "next to", "above", "below"
   * comparative relationships: "more than", "bigger than", "compare"
   * quantitative relationships: "how many", "count", "most", "least"
   * temporal relationships: "before", "after", "during", "sequence"

2. compute specialized metrics

   * relational_accuracy = correct relational answers / total relational questions
   * non_relational_accuracy = correct non-relational answers / total non-relational questions
   * relational_hallucination_rate = hallucinated relational answers / total relational questions

3. comparative analysis

   * performance gap between relational vs non-relational questions
   * dataset-specific relational reasoning capabilities
   * embedding layer analysis for relational vs non-relational processing

4. write enhanced scores with relational analysis

   * question_type_performance, relational_reasoning_score, spatial_accuracy, comparative_accuracy

notes

* the pre-curated relational labels eliminate the need for complex parsing and reduce false positives
* focus on understanding where models struggle with relational reasoning vs factual recall

---

## tips and troubleshooting

* memory spikes during VQA extraction: lower --chunk-size or use the gpu script
* persistent cuda oom: set torch dtype to fp16/bf16 if supported
* missing images: ensure image names in VQA dataset match files in all_images directory
* relational accuracy issues: verify is_relational flags are correctly preserved through pipeline
* reproducibility: embeddings hdf5 files include model_name and selected_layers as file attributes for traceability
* dataset validation: use the 10k sample with question_ids for consistent tracking across pipeline stages

---
